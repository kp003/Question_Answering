
#Name:Pawan Khadka
#Date:05/06/2021
#Program:Question Generation

#importing necessary libraries
import sys
import nltk
#nltk.download('punkt')
from nltk.tree import Tree as Tree
import nltk
#nltk.download('wordnet')
# nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from lemminflect import getInflection
from stanfordcorenlp import StanfordCoreNLP
#nltk.download('averaged_perceptron_tagger')
from nltk.tree import Tree as Tree
import spacy
import random
import numpy as np
nlp1 = spacy.load("en_core_web_sm")

import numpy as np

nlp = StanfordCoreNLP('stanford-corenlp-4.2.1',lang='en')

#this function will return the tokenized article/dataset
def dataset_creation(text):
  extra_abbreviations = ['dr', 'vs', 'mr', 'mrs', 'prof', 'inc', 'i.e','e.g']
  sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
  sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)
  tokenized_sentence=sentence_tokenizer.tokenize(text)
  return tokenized_sentence

#this function checks if a sentence has a verb phrase and a noun phrase
def check_binary(tree):
  Count_NP=0
  Count_VP=0
  sentence=[]
  for subtree in tree[0]:
    if subtree.label()=='NP':
      Count_NP=1
    if subtree.label()=='VP':
      Count_VP=1
    if subtree.label()!='VP':
      sentence+= (subtree.leaves())
    else:
      for subtree_in_VP in subtree:
        sentence+= (subtree_in_VP.leaves())
  return Count_NP,Count_VP

#this function groups words in the verb phrase from the parse tree
def group_words(tree):
 sentence_label=[]
 new_sentence=[]
 result=[]
 for subtree in tree[0]:
   if subtree.label()!='VP':
     sentence_label.append(subtree.label())
     new_sentence.append(" ".join(subtree.leaves()))
   else:
     sentence_label.append('VB')
     for t in subtree:
       result.append(" ".join(t.leaves()))
     new_sentence+=result
 return(new_sentence,sentence_label)

#this list contains all the binary questions generated by the system
list_of_q=[]

#this list contains questions that can be helpful in generating when_questions
could_be_when_questions=[]

#this function perfoms noun-phrase, verb inversion and updates the question list
def print_result(new_sentence,sentence_label):
  Model_verb=['can','could','may','might','will','would','shall','should','must']
  Be_verb=['is','am','are','was','were']
  verb_index=sentence_label.index('VB')
  #verb=new_sentence[verb_index].capitalize()
  verb=new_sentence[verb_index]
  np_index=sentence_label.index('NP')
  noun=new_sentence[np_index]
  flag_=1
#switch NP with Verb
#if verb is in the list:
  if (verb in Be_verb) | (verb in Model_verb):   
    new_sentence[verb_index]=noun
    #removed capitalize from the verb
    new_sentence[np_index]=verb
    new_sentence[-1]='?'
  else:
    verb_length=len(new_sentence[0].split(" "))
    original_sentence=" ".join(new_sentence)
    result=nltk.pos_tag(word_tokenize(original_sentence))
    verb_tag=result[verb_length][1]

    if verb_tag=='VBD':
#####  #replace V3 with V1
      new_sentence[verb_index]=WordNetLemmatizer().lemmatize(new_sentence[verb_index],'v')
      new_sentence[-1]='?'
      new_sentence.insert(np_index,'did')
  
    elif verb_tag=='VBZ':
#####  #replace V3 with V1
     new_sentence[verb_index]=WordNetLemmatizer().lemmatize(new_sentence[verb_index],'v')
     new_sentence[-1]='?'
     new_sentence.insert(np_index,'does')

    elif verb_tag=='VBP':
 #replace V3 with V1
     new_sentence[verb_index]=WordNetLemmatizer().lemmatize(new_sentence[verb_index],'v')
     new_sentence[-1]='?'
     new_sentence.insert(np_index,'do')

    else:
      #could possible return null
      flag_=0

  if (flag_):
   new_sentence=" ".join(new_sentence)
   #print(new_sentence)
   list_of_q.append(new_sentence)

#this function reads files,calls functions to update the main question list
def generate_questions(article):
  f=open(article)
  #print("fileopened")
  docpassed=f.read()
  #stanford parse was failing with % sign,so replaced it with percentage
  docpassed = docpassed.replace("%", 'percentage')
  dataset1=dataset_creation(docpassed)
  #print("dataset created")
  #print(dataset1)
  for sentence in dataset1:
    #print(sentence)
    #used try..except to catch error that stanford parser was having with certain symbols
    try:
      tree_str=nlp.parse(sentence)
      #print(sentence)
      tree=Tree.fromstring(str(tree_str))
      #print(tree)
      count_np,count_vp=check_binary(tree)
      #print(count_np and count_vp)

      if count_np and count_vp:
        #print("inside if of loop")
        new_sentence,sentence_label=group_words(tree)
        print_result(new_sentence,sentence_label)
    except:
      #print("Sentence could not be parsed")
      #print(sentence)
      continue
    #print("completed for loop for the sentence")

#this function produces when.....? questions based on different patterns/condidtions

def where_when_question(sen,wh_word):
  tree_str=nlp.parse(sen)
  tree=Tree.fromstring(str(tree_str))
  #most common time preposition to locate time/data phrase
  list_of_time_prep=['in','on','at','since','between','during']
  list_of_loc_prep=['in','on','by','across']

  preposition_phrase=[]

  for subtree in tree[0]:
   if subtree.label()=='VP':
    for t in subtree:
      if t.label()=='PP':
        preposition_phrase.append(" ".join(t.leaves()))

  preposition_phrase=" ".join(preposition_phrase)
  if len(preposition_phrase)>1:
    to_remove_words=""

    if wh_word=="When":
      prep_list=list_of_time_prep
    else:
      prep_list=list_of_loc_prep

    #print(prep_list)
    #print(preposition_phrase)

    if preposition_phrase.split()[0]  in prep_list:
      #print(preposition_phrase.split()[0])
      #print(sen)
      sen = sen.replace(preposition_phrase, ' ')
      #print(sen)
      sen=wh_word.capitalize()+" "+sen
      return sen
    else:
      for word in preposition_phrase.split():
        if word not in prep_list:
          to_remove_words=to_remove_words+" "+word
        else:
          #print(to_remove_words)
          #preposition_phrase.replace(to_remove_words,'')
          #print(preposition_phrase)
          sen=sen.replace(preposition_phrase,to_remove_words)
          #print(sen)
          sen=wh_word.capitalize()+" "+sen
          return sen
  trend1=[]
  for subtree in tree[0]:
    if subtree.label()=="WHNP":
      trend1.append(" ".join(subtree.leaves()))
  trend1=" ".join(trend1)
  if len(trend1)>1:
    sen=sen.replace(trend1,"When")
    return sen
  trend2=[]
  for subtree in tree[0]:
    if subtree.label()=="PP":
      trend2.append(" ".join(subtree.leaves()))
  trend2=" ".join(trend2)
  if (len(trend2)>1):
    sen=sen.replace(trend2,"When")
    return sen
  trend3=[]
  for subtree in tree[0]:
    if subtree.label()=="SBAR":
      trend3.append(" ".join(subtree.leaves()))
  trend3=" ".join(trend3)
  if len(trend3)>1:
    sen=sen.replace(trend3,"When")
    return sen
  trend4=[]
  for subtree in tree[0]:
    if subtree.label()=="S":
      for t in subtree:
        #print(t.label())
        if (t.label()=="VP"):
          for t_final in t:
            if (t_final.label()=="PP"):
              trend4.append(" ".join(t_final.leaves()))
  trend4=" ".join(trend4)
  if len(trend4)>1:
    sen=sen.replace(trend4," ")
    sen="When"+" "+sen
    return sen
  return sen

#NP-NP-proper noun
#what="Is Paris the most beautiful city in the world?"
#this function produces what....? questions based on different patterns
def what_sentence(sentence):
  doc_q=nlp1(sentence)
  for t in doc_q.ents:
    if (t.label_=="DATE" or t.label_=="TIME"):
      could_be_when_questions.append(sentence)
  tree_str=nlp.parse(sentence)
  tree=Tree.fromstring(str(tree_str))
  tree_pattern=[]
  for subtree in tree[0]:
    tree_pattern.append(subtree.label())
  pattern_1=['VBZ','NP',',','PRN',',','NP','.']
  if tree_pattern==pattern_1:
    to_remove_np=[]
    index=1
    for subtree in tree[0]:
      if index==1:
        if subtree.label()=="NP":
          index=0
      else:
        if subtree.label()=="NP":
          to_remove_np.append(" ".join(subtree.leaves()))

    to_remove_np="".join(to_remove_np)

    sentence=sentence.replace(to_remove_np,"")
    sentence=sentence.replace(",","")
    tree_str=nlp.parse(sentence)
    tree=Tree.fromstring(str(tree_str))

   
  new_sentence=[]
  proper_noun_pos=["NNP","NNPS"]
  index=0
  for subtree in tree[0]:
    #index=0
    if subtree.label()=="NP":
      #print("subtree leaves with NP")
      #print(subtree.leaves())
      for t in subtree:
        #print(t.label())
        if t.label() == "NP":
          for t_f in t:
            if t_f.label() in proper_noun_pos:
              index=1
      if index==1:
        new_sentence.append(" ".join(subtree.leaves()))
        new_sentence=" ".join(new_sentence)
        #print("new_sentence")
        #print(new_sentence)
        return new_sentence,sentence

  #NP-proper noun
  for subtree in tree[0]:
    #index=0
    if subtree.label()=="NP":
      for t in subtree:
        if t.label() in proper_noun_pos:
          index=1
      if index==1:
        new_sentence.append(" ".join(subtree.leaves()))
        new_sentence=" ".join(new_sentence)
        #print("new_sentence")
        #print(new_sentence)
        return new_sentence,sentence
  return sentence,sentence

#given a sentence, this functions returns the pos_tag of verb and the verb itself
def get_verb(question):

  tree_str=nlp.parse(question)
  tree=Tree.fromstring(str(tree_str))
  vb=["VB","VBP","VBD"]
  for subtree in tree[0]:
    if subtree.label()=="VP":
      for t in subtree:
        if t.label() in vb:
          return t.leaves(),t.label()
        else:
          if t.label()=="VP":
            for t_f in t:
              if t_f.label() in vb:
                return t_f.leaves(),t_f.label()
  return "not found","not found"

#this function produces who...?question replacing PERSON entity with the word who
def who_question(question):
  doc_q=nlp1(question)
  person_name=""
  for t in doc_q.ents:
    if (t.label_=="DATE" or t.label_=="TIME"):
      could_be_when_questions.append(question)

  for t in doc_q.ents:
    if (t.label_=='PERSON'):
      person_name=str(t)
    else:
      continue
    break
  Do_verb=["did","does"]
  if question.split()[0] not in Do_verb:
    #print(question.find(person_name))
    question = question.replace(person_name,"")
    question = "Who"+" "+question
    
    return question.strip()
  else:
    verb,label=get_verb(question)
    if verb=="not found":
      return question
    if question.split()[0]=="did":
      if label != "VBD":
        changed_verb=getInflection(verb[0], tag='VBD')[0]
      else:
        changed_verb=verb[0]
      question=question.replace("did","")
    else:
      question=question.replace("does","")
      changed_verb=getInflection(verb[0],tag='VBZ')[0]
    question=question.replace(verb[0],changed_verb)
    question=question.replace(person_name,"")
    question="Who"+question
    return question.strip()

#the main function
def main():
  filename=sys.argv[1]
  number_of_questions=int(sys.argv[2])
  #filename="/content/a7.txt"
  #number_of_questions=30
  generate_questions(filename)
  new_questions=[]
  for i in list_of_q:
    doc=nlp1(i)
    for t in doc.ents:
      if(t.label_):
        new_questions.append(i)
  x = np.array(new_questions)
  #print(len(np.unique(x)))
  unique_question=np.unique(x)

  #could_be_when_questions=[]
  binary_questions=[]
  who_questions=[]
  when_questions=[]
  what_questions=[]
  #looping over each question to generate 
  unique_question_list=unique_question.tolist()
  words_to_remove_from_qn=['it','It','he','He','She','she','they','They','that','That']
  for x in unique_question_list:
    if x.split()[1] in words_to_remove_from_qn:
      unique_question_list.remove(x)
  verb_l=["Do","Did","Does"]
  for question in unique_question_list:
    #print(question)
    que=nlp1(question)
    for t in que.ents:
      if (t.label_=="PERSON"):
        #print("who")
        final_question=who_question(question)
        if final_question==question:
          binary_questions.append((" ".join(question.split())))
        else:
          who_questions.append((" ".join(final_question.split())))
      
      elif (t.label_=="DATE" or t.label_=="TIME"):
        final_question=where_when_question(question,"When")
        #print("when")
        if final_question==question:
          binary_questions.append((" ".join(question.split())))
        else:
          when_questions.append((" ".join(final_question.split())))
      # elif (t.label_=="LOC"or t.label_=="GPE"):
      #   question=where_when_question(question,"Where")
      #   print(question)
      else:
       # print("no change required")
       # print(question)
        li=['do','did','does']
        if question.split()[0] not in li:
          result,question = what_sentence(question)
          if result!=question:
            question=question.replace(result," ")
            question="What"+" "+question
            what_questions.append((" ".join(question.split())))
          else:
            binary_questions.append((" ".join(question.split())))
          
      break
  for question in could_be_when_questions:
    final_question=where_when_question(question,"When")
    if final_question != question:
      when_questions.append(final_question)
  #improve questions:

  #improving when question
  for x in when_questions:
    if x.split()[0]!="When":
      when_questions.remove(x)
    

  #improving who question
  for x in who_questions:
    doc=nlp1(x)
    pos_of_words=['AUX','VERB']
    if doc[1].pos_ not in pos_of_words:
      who_questions.remove(x)

  #improving what questions
  words_that_follow_what=['is','are','were','and','do','did','does']
  for x in what_questions:
    if x.split()[1] not in words_that_follow_what:
      what_questions.remove(x)

  #imrpoving binary questions
  binary_questions=[x.capitalize()for x in binary_questions]


#sorting all the questions based on their length
  binary_questions.sort(key=len)
  when_questions.sort(key=len)
  who_questions.sort(key=len)
  what_questions.sort(key=len)


  bank_of_questions=[]

  bank_of_questions.append(when_questions)
  bank_of_questions.append(who_questions)
  bank_of_questions.append(what_questions)

  bank_of_questions.sort(key=len)

  final_question_bank=[]


#before adding all the other questions to the final_list_of questions
#adding atleast 30% bianry questions since, binary questions are well formed/grammatically correct

  no_of_bianry_qn = int(0.3* number_of_questions)

  for x in range(no_of_bianry_qn):
    if x < len(binary_questions):
      final_question_bank.append(binary_questions[x])

  if len(final_question_bank)!= no_of_bianry_qn:
    remaining_qn=no_of_bianry_qn-len(final_question_bank)
    for x in range(remaining_qn):
      if len(unique_question_list)>x:
        if unique_question_list[x] not in final_question_bank:
          final_question_bank.append(unique_question_list[x])
  #appending starting from the least produced quetsion type,after appending binary questions
  for x in range(len(bank_of_questions[2])):
    if x<len(bank_of_questions[0]):
      final_question_bank.append(bank_of_questions[0][x])
    if x<len(bank_of_questions[1]):
      final_question_bank.append(bank_of_questions[1][x])
    if x<len(bank_of_questions[2]):
      final_question_bank.append(bank_of_questions[2][x])

  #remove if any duplicates are present

  final_questions_to_output = [] 
  for x in final_question_bank :
   if x not in final_questions_to_output:
     final_questions_to_output.append(x)
  final_list=[]
  if len(final_questions_to_output)>= number_of_questions:
    #Get the top n and shuffle
    for x in range(number_of_questions):
      final_list.append(final_questions_to_output[x])

  else:
    for x in binary_questions:
      if x not in final_questions_to_output:
        final_questions_to_output.append(x)
    if len(final_questions_to_output)<number_of_questions:
      for x in unique_question_list:
        if x not in final_questions_to_output:
          final_questions_to_output.append(x)
    for x in range(number_of_questions):
      if len(final_questions_to_output)>x:
        final_list.append(final_questions_to_output[x])

 #shuffling the questions 
  random.shuffle(final_list)
  for x in final_list:
    print(x)

if __name__ == "__main__":
  main()
